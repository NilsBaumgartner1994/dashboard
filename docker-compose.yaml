services:
  my-dashboard-tts-cpu:
    build:
      context: ./apps/tts
      target: cpu-base
    image: qwen3-tts-api-cpu
    restart: always
    profiles:
      - cpu
    networks:
      - directus_network
    ports:
      - "8880:8880"
    volumes:
      - ./data/tts-models:/root/.cache/huggingface
    environment:
      TTS_MODEL_ID: "${TTS_MODEL_ID:-Qwen/Qwen3-TTS-12Hz-0.6B-Base}"
      # Optional: set HUGGING_FACE_HUB_TOKEN in .env for private/gated models.
      # Not required for the default public model.
      HUGGING_FACE_HUB_TOKEN: "${HUGGING_FACE_HUB_TOKEN:-}"
    mem_limit: ${TTS_MEMORY:-4g}

  my-dashboard-ai:
    image: ollama/ollama:latest
    restart: always
    networks:
      - directus_network
    volumes:
      - ./data/ollama:/root/.ollama
    environment:
      OLLAMA_KEEP_ALIVE: "24h"
      # Enable flash attention: reduces KV-cache memory and speeds up inference on CPU
      OLLAMA_FLASH_ATTENTION: "1"
      # Quantize the KV cache to q8_0 to cut memory bandwidth and improve token throughput
      OLLAMA_KV_CACHE_TYPE: "${OLLAMA_KV_CACHE_TYPE:-q8_0}"
      # Limit parallel requests to 1 so the single model run gets all available threads
      OLLAMA_NUM_PARALLEL: "${OLLAMA_NUM_PARALLEL:-1}"
    mem_limit: ${OLLAMA_MEMORY:-8g}
    # Pull the model on startup if not already present
    entrypoint: ["/bin/sh", "-c", "ollama serve & sleep 5 && ollama pull ${OLLAMA_MODEL:-llama3.1:8b} && wait"]

  prepare-data:
    image: debian:bookworm-slim
    volumes:
      - ./preparePermission.sh:/preparePermission.sh
      - ./data:/data
      - .env:/.env
    working_dir: /
    entrypoint: ["/bin/sh", "-c", "chmod +x ./preparePermission.sh && ./preparePermission.sh"]

#  traefik:
#    env_file:
#      - .env
#    extends:
#      service: traefik
#      file: apps/proxy/docker-compose.yaml

  my-dashboard-directus:
    env_file:
      - .env
    depends_on:
      prepare-data:
        condition: service_completed_successfully
      my-dashboard-cache:
        condition: service_started
      my-dashboard-ai:
        condition: service_started
    environment:
      OLLAMA_URL: "http://my-dashboard-ai:11434"
      TTS_URL: "http://my-dashboard-tts-cpu:8880"
    extends:
      service: my-dashboard-directus
      file: apps/backend/docker-compose.yaml

  my-dashboard-database-sync:
    env_file:
      - .env
    depends_on:
      prepare-data:
        condition: service_completed_successfully
    extends:
      service: my-dashboard-database-sync
      file: apps/backend/docker-compose.yaml

  my-dashboard-cache:
    env_file:
      - .env
    extends:
      service: my-dashboard-cache
      file: apps/backend/docker-compose.yaml
    networks:
      - directus_network

networks:
  directus_network: {}
  traefik:
    driver: bridge
    name: traefik
