RESTART=always

# For Deployment
#DOMAIN_PRE=https
#MYHOST=nilsbaumgartner.de

# For Development
DOMAIN_PRE=http
MYHOST=127.0.0.1

DOMAIN_PATH=my-dashboard

BACKEND_PATH=api
BACKEND_PORT=8055
BACKEND_CACHING=false

# AI Agent (Ollama) – set the model to download/use on startup
# No API key required; the model is downloaded from the Ollama registry on first run.
# Requires ~4-5 GB of disk space for llama3.1:8b
OLLAMA_MODEL=llama3.1:8b
# Memory limit for the Ollama container.
# Automatically set by scripts/configure-ram.sh (80 % of host RAM, min 2 GB).
# You can also set this manually, e.g. OLLAMA_MEMORY=8g
OLLAMA_MEMORY=8g

# ── Ollama performance tuning ─────────────────────────────────────────────────
# Context window passed to the model on every request.
# llama3.1 defaults to 131 072 tokens which is very slow on CPU.
# 4096 tokens is sufficient for most chat sessions and gives a ~10–20× speed-up.
# Increase only if you need long conversation history (uses more RAM + CPU time).
OLLAMA_NUM_CTX=4096

# KV-cache quantisation – q8_0 cuts memory bandwidth while keeping quality high.
# Use q4_0 for maximum throughput on heavily loaded machines.
# Set to f16 to disable quantisation (highest quality, most memory).
OLLAMA_KV_CACHE_TYPE=q8_0

# Maximum parallel inference requests. Keep at 1 on a shared CPU host so the
# single active request gets all available threads instead of competing.
OLLAMA_NUM_PARALLEL=1

# CPU threads used by Ollama for inference. Leave empty to let Ollama auto-detect.
# On a 6-core VPS running multiple services, setting this to 4 leaves 2 cores
# free for Postgres/Node and reduces scheduling contention.
# OLLAMA_NUM_THREADS=4

# Optional: provide a Hugging Face token if you want to download gated models
# HUGGING_FACE_TOKEN=hf_...
