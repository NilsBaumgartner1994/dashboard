RESTART=always

# For Deployment
#DOMAIN_PRE=https
#MYHOST=nilsbaumgartner.de

# For Development
DOMAIN_PRE=http
MYHOST=127.0.0.1

DOMAIN_PATH=my-dashboard

BACKEND_PATH=api
BACKEND_PORT=8055
BACKEND_CACHING=false

# AI Agent (Ollama) – set the model to download/use on startup
# No API key required; the model is downloaded from the Ollama registry on first run.
# Requires ~4-5 GB of disk space for llama3.1:8b
OLLAMA_MODEL=llama3.1:8b
# Memory limit for the Ollama container.
# Automatically set by scripts/configure-ram.sh (80 % of host RAM, min 2 GB).
# You can also set this manually, e.g. OLLAMA_MEMORY=8g
OLLAMA_MEMORY=8g

# ── Ollama performance tuning ─────────────────────────────────────────────────
# Context window passed to the model on every request.
# llama3.1 defaults to 131 072 tokens which is very slow on CPU.
# 4096 tokens is sufficient for most chat sessions and gives a ~10–20× speed-up.
# Increase only if you need long conversation history (uses more RAM + CPU time).
OLLAMA_NUM_CTX=4096

# KV-cache quantisation – q8_0 cuts memory bandwidth while keeping quality high.
# Use q4_0 for maximum throughput on heavily loaded machines.
# Set to f16 to disable quantisation (highest quality, most memory).
OLLAMA_KV_CACHE_TYPE=q8_0

# Maximum parallel inference requests. Keep at 1 on a shared CPU host so the
# single active request gets all available threads instead of competing.
OLLAMA_NUM_PARALLEL=1

# CPU threads used by Ollama for inference. Leave empty to let Ollama auto-detect.
# On a 6-core VPS running multiple services, setting this to 4 leaves 2 cores
# free for Postgres/Node and reduces scheduling contention.
# OLLAMA_NUM_THREADS=4

# Optional: provide a Hugging Face token if you want to download gated models
# HUGGING_FACE_TOKEN=hf_...

# Docker socket path used by backend docker-logs endpoint.
# Requires mounting /var/run/docker.sock into the directus container.
DOCKER_SOCKET_PATH=/var/run/docker.sock

# ── Qwen3-TTS (Sprachausgabe) ─────────────────────────────────────────────────
# Model is downloaded automatically from Hugging Face Hub on the first start.
# The default model (Qwen3-TTS-12Hz-0.6B-Base) is PUBLIC – no token required.
# Files are cached in ./data/tts-models/ and reused on subsequent starts (~1-2 GB).
# Start with: docker-compose --profile cpu up my-dashboard-tts-cpu

# Default HuggingFace model ID used for synthesis when no model is provided.
TTS_MODEL_ID=Qwen/Qwen3-TTS-12Hz-0.6B-Base

# Comma-separated list of models to preload and keep available in the CPU container.
# Includes both requested variants by default.
TTS_MODEL_IDS=Qwen/Qwen3-TTS-12Hz-0.6B-Base,Qwen/Qwen3-TTS-1.7B-Base

# Explicit default model from the loaded list.
TTS_DEFAULT_MODEL_ID=Qwen/Qwen3-TTS-12Hz-0.6B-Base

# Memory limit for the TTS container.
# Automatically set by scripts/configure-ram.sh (30 % of host RAM, min 2 GB).
# You can also set this manually, e.g. TTS_MEMORY=4g
TTS_MEMORY=4g

# Optional: Hugging Face token – only needed for private or gated models.
# Leave empty (or commented out) when using the default public model.
# HUGGING_FACE_HUB_TOKEN=hf_...
